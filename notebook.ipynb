{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "test_zf = zipfile.ZipFile('data//test.csv.zip')\n",
    "train_zf = zipfile.ZipFile('data//train.csv.zip')\n",
    "\n",
    "train = pd.read_csv(train_zf.open('train.csv'))\n",
    "test = pd.read_csv(test_zf.open('test.csv'))\n",
    "\n",
    "train['train'] = 1\n",
    "test['train'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.describe())\n",
    "print(train.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['fruitset', 'fruitmass', 'seeds']\n",
    "fig = plt.figure(figsize = (12, 8))\n",
    "for i, var in enumerate(vars):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.grid(b=True, axis='y')\n",
    "\n",
    "    plt.title(f\"{var} vs yield\", size=18, y=1.03, fontname='Calibri', \n",
    "                fontweight='bold', color='#444444')\n",
    "    ax1 = sns.scatterplot(data = train, x=var, y='yield', hue=\"clonesize\", alpha= 0.5)\n",
    "    ax1.set(ylim=(0, 10000))\n",
    "    plt.ylabel('Yield')\n",
    "    plt.xlabel(var)\n",
    "    plt.xticks(fontname='Calibri', size=12)\n",
    "    plt.yticks(fontname='Calibri', size=12)\n",
    "    for s in ['right', 'top', 'left', 'bottom']:\n",
    "        if s == \"bottom\":\n",
    "            ax1.spines[s].set_linewidth(2)\n",
    "        else:\n",
    "            ax1.spines[s].set_visible(False)\n",
    "\n",
    "    fig.tight_layout(pad=3)\n",
    "\n",
    "    plt.subplot(2,3,i+4)\n",
    "    plt.grid(b=True, axis='y')\n",
    "\n",
    "    plt.title(f\"{var} distribution\", size=18, y=1.03, fontname='Calibri', \n",
    "                fontweight='bold', color='#444444')\n",
    "    ax2 = sns.histplot(data = train, x=var, color='#DB97D5')\n",
    "    ax2.set(ylim=(0, 1000))\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel(var)\n",
    "    plt.xticks(fontname='Calibri', size=12)\n",
    "    plt.yticks(fontname='Calibri', size=12)\n",
    "    for s in ['right', 'top', 'left', 'bottom']:\n",
    "        if s == \"bottom\":\n",
    "            ax2.spines[s].set_linewidth(2)\n",
    "        else:\n",
    "            ax2.spines[s].set_visible(False)\n",
    "\n",
    "    fig.tight_layout(pad=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['clonesize', 'honeybee', 'bumbles', 'andrena', 'osmia', 'RainingDays', 'AverageRainingDays']\n",
    "fig = plt.figure(figsize = (12, 12))\n",
    "for i, var in enumerate(vars):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.grid(b=True, axis='y')\n",
    "\n",
    "    plt.title(f\"{var} vs yield\", size=18, y=1.03, fontname='Calibri', \n",
    "                fontweight='bold', color='#444444')\n",
    "    ax1 = sns.scatterplot(data = train, x=var, y='yield', alpha= 0.5,color='#DB97D5')\n",
    "    ax1.set(ylim=(0, 10000))\n",
    "    plt.ylabel('Yield')\n",
    "    plt.xlabel(var)\n",
    "    plt.xticks(fontname='Calibri', size=12)\n",
    "    plt.yticks(fontname='Calibri', size=12)\n",
    "    for s in ['right', 'top', 'left', 'bottom']:\n",
    "        if s == \"bottom\":\n",
    "            ax1.spines[s].set_linewidth(2)\n",
    "        else:\n",
    "            ax1.spines[s].set_visible(False)\n",
    "\n",
    "    fig.tight_layout(pad=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperatures showed low correlations with yield (see correlation matrix). I came to the conclusion that none these features can be removed from the data set. Some of the variables were binned using k-means clustering. Outlier were filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = train[train['honeybee'] < 4]\n",
    "train = train[train['bumbles'] < 0.5]\n",
    "train = train[train['andrena'] > 0.2]\n",
    "\n",
    "combined = pd.concat([train, test])\n",
    "combined['avg_bees'] = (combined['bumbles'] + combined['andrena'] + combined['osmia']) \n",
    "combined.drop(combined.columns[combined.columns.str.contains('TRange')],\n",
    "            axis=1, inplace=True)\n",
    "combined = combined.drop('RainingDays', axis=1)\n",
    "\n",
    "helper_dict = {'clonesize' : 4,\n",
    "               'honeybee' : 3,\n",
    "               'bumbles' : 3,\n",
    "               'andrena' : 5,\n",
    "               'osmia' : 6,\n",
    "               'AverageRainingDays' : 4}\n",
    "\n",
    "\n",
    "for key, value in helper_dict.items():\n",
    "    km = KMeans(n_clusters=value, n_init=10).fit(train[[key]])\n",
    "    df = pd.DataFrame(list(zip(range(len(km.cluster_centers_)), km.cluster_centers_)),\n",
    "                  columns=['labels', 'centers'])\n",
    "    df = df.sort_values(['centers'])\n",
    "    new_column = key + \"_labels\"\n",
    "    combined[new_column] = km.predict(combined[[key]])\n",
    "    combined[new_column] = combined[new_column].map(dict(zip(df['labels'].to_list(),\n",
    "                                                              range(len(df.index)))))\n",
    "    combined = combined.drop(key, axis=1)\n",
    "\n",
    "combined = combined.drop(columns=['id'])\n",
    "X = combined[combined['train'] == 1]\n",
    "X.pop('train')\n",
    "Y = X.pop('yield')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperpareter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'boosting_type': 'gbdt', 'max_depth': -1, 'objective': 'regression', \n",
    "#               'num_leaves': 64, 'learning_rate': 0.05, 'max_bin': 512, \n",
    "#               'subsample_for_bin': 200, 'subsample': 1, 'subsample_freq': 1,\n",
    "#               'colsample_bytree': 0.8, 'reg_alpha': 5, 'reg_lambda': 10, \n",
    "#               'min_split_gain': 0.5, 'min_child_weight': 1, \n",
    "#               'min_child_samples': 5, 'scale_pos_weight': 1, 'num_class': 1, \n",
    "#               'metric': 'mae'}\n",
    "\n",
    "# grid_params = {'learning_rate': [0.01], 'n_estimators': [8, 24],\n",
    "#                 'num_leaves': [6, 8, 12, 16], 'boosting_type': ['gbdt'], \n",
    "#                 'objective': ['regression'], 'seed': [500],\n",
    "#                 'colsample_bytree': [0.65, 0.75, 0.8], \n",
    "#                 'subsample': [0.7, 0.75], 'reg_alpha': [1, 2, 6],\n",
    "#                 'reg_lambda': [1, 2, 6]}\n",
    "\n",
    "\n",
    "# mod = lgb.LGBMClassifier(**params)\n",
    "# print(mod.get_params().keys())\n",
    "# grid = GridSearchCV(mod, param_grid=grid_params, verbose=1, cv=5, n_jobs=-1)\n",
    "# grid.fit(X, Y)\n",
    "\n",
    "# best_params = {k: grid.best_params_.get(k, v) for k, v in params.items()}\n",
    "# best_params['verbosity'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "kf = KFold(n_splits = folds, shuffle = True, random_state = 43)\n",
    "kf.get_n_splits(X)\n",
    "mae = 0\n",
    "\n",
    "for train_index, valid_index in kf.split(X):\n",
    "    \n",
    "    x_train, x_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = Y.iloc[train_index], Y.iloc[valid_index]\n",
    "\n",
    "    lgb_train = lgb.Dataset(x_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "    best_params = {\n",
    "        'objective': 'mae', \n",
    "        'num_leaves': 400,  \n",
    "        'min_child_weight': 14, \n",
    "        'max_depth': 7, \n",
    "        'learning_rate': 0.05, \n",
    "        'force_col_wise': True, \n",
    "        'colsample_bytree': 0.9,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(best_params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=lgb_valid,\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=5)])\n",
    "    \n",
    "    y_pred = gbm.predict(x_valid, num_iteration=gbm.best_iteration)\n",
    "    mae += mean_absolute_error(y_valid, y_pred)\n",
    "\n",
    "print(mae/folds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
